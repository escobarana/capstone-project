name: Upload Airflow DAGs to S3
on:
  push:
    branches:
      - '*'
  pull_request:
    branches: [ main ]

jobs:
  upload_dags:
    name: Upload Airflow DAGs to S3
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: .
    steps:
      - name: Git checkout
        uses: actions/checkout@v3

      - name: Set up S3cmd cli tool
        uses: s3-actions/s3cmd@v1.4.0
        with:
          provider: aws # default is linode
          region: ${{ secrets.AWS_REGION_NAME }}
          access_key: ${{ secrets.AWS_ACCESS_KEY_ID }}
          secret_key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}

      - name: Interact with object storage
        run: |
          s3cmd put dags/batch_job.py --mime-type 'application/x-python-code' --acl-public s3://dataminded-academy-capstone-resources/dags/airflow_ana.py
